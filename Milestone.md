# Прогресс по проекту

## Задача:

обучить модель, генерирующую хорошие и подробные описания мемов (задача Image Captioning)

## Данные

### Описания мемов

Были скачаны данные из групп вк textmeme и badtextmeme, лежат [тут](data/memes). 
К моему сожалению, получилось очень мало пар картинка-текст (~3500). 

Примеры описаний:

1. На фото котик сидит за обеденным столом на стуле. Перед ним стоит свежезаваренный чай и лежат куски пиццы на тарелках. Подпись: "вот это доброе утро пицца на завтрак чаёк тока жаль любимого человека нет один сижу"

<img src="data/memes/images/58.jpg" width="350">

2. На фотографии яичница. На желтке изображена полупрозрачная морда кота. Он смотрит на читателя мема. Подпись: "Чем долбше находишбся в иллюзиях, тем болбнее вернутса в реальностб.."

<img src="data/memes/images/479.jpg" width="350">


Если кратко описывать данные - то это пары картинка-текст, где картинки - это мемы 
(фото, фото с подписями и просто скрины текстов/постов), а текст - довольно подробное описание картинки, 
включающее в себя описание происходящего на фото/картинке, а также подписи. Описания не очень длинные, 
но длиннее, чем в классических датасетах, и они не обязательно состоят из одного предложения. 


### Дополнительные данные

В связи с тем, что данных с описаниями мемов оказалось крайне мало, я начала искать варианты 
схожих датасетов, и у меня не удалось найти для русского языка датасетов с парами картинка-текст.

Возможно, для чего-то очень сильно предобученного мой 
маленький датасет сгодится в качестве дообучения, но было принято решение
перевести какие-нибудь данные с английского. Выбор пал на [Flickr30k](https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset), 
я не искала сложных путей и просто перевела предложения на русский 
в Google Sheets, где используется Google Translate. 
Всего было переведено 47220 предложений, которые соответствовали картинкам. 

Еще хотела скачать картинки из датасета Conceptual captions, 
так как перевела таким же образом валидационную часть датасета (около ~16k картинок). 
Но картинки по ссылкам скачивались очень долго, из-за чего от этой идеи пришлось отказаться.

Уже после обучения первого бейзлайна удалось найти еще один хороший датасет на английском языке, 
подходящий для моей задачи даже больше, чем предыдущие, - это
[VizWiz](https://vizwiz.org/tasks-and-datasets/image-captioning/). Это также датасет с парами картинка-текст,
но он состоит из фото, которые были сделаны людьми с нарушением зрения, а описания составлены таким образом, чтобы объяснить такому человеку происходящее на картинке.
Другими словами, датасет содержит более подробные описания, чем Flickr или COCO и другие схожие датасеты.
Я скачала обучающую и валидационную части этого датасета, и перевела их. В валидационной части получилось 
~33500 пар, а в обучающей - . Этот датасет использовался для обучения второго безйлайна.

Стоит отметить, что перевод описаний не идеален - там бывают ошибки различного рода, например, грамматические.
Однако, на мое субъективное мнение, полученные описания неплохи, к тому же, у меня не было особо другого выбора.

## Метрики

В качесте метрик оценки моделей были выбраны BLEU-1,  BLEU-2,  BLEU-3,  BLEU-4 и METEOR. 
(BLEU отлючаются между собой по кол-ву n-грамм, которые сравниваются).

## Обучение моделей

В целом, задача Image Captioning обычно решается с помощью архитектуры Encoder-Decoder, где encoder кодирует 
представление картинки, а decoder пытается раскодировать его в описание этой самой картинки.

### Baseline 1: ResNet-50 + LSTM

В качестве простой бейзлайновой модели выбрала связку ResNet-50 + LSTM, код написан по примеру 
[отсюда](https://github.com/sauravraghuvanshi/Udacity-Computer-Vision-Nanodegree-Program/tree/master/project_2_image_captioning_project)
и его можно увидеть [здесь](src/baseline/train_baseline.py). ResNet взят предобученным и не дообучается в процессе обучения модели.


Данные токенизировались с помощью nltk.word_tokenize и приводились к нижнему регистру. Одно описание картинки - одна последовательность,
максимальная длина = 100 слов. В случае, если последовательность длиннее - она обрезается до 100 слов, если короче - дополняется паддингами.

Обучались только эмбеддинги слов, батч-нормализация в encoder и сам decoder. 
Скрытая размерность LSTM=512, размер эмбеддингов картинки и текста=512. В качестве функции потерь
использовалась кросс-энтропия, оптимизатор для обноваления весов функции - Adam с параметрами lr=0.001, betas=(0.9, 0.999), eps=1e-08.
После каждого bask-propagation использовался клиппинг градиентов по норме до 5.

Во-первых, сначала модель в течение 30 эпох обучалась на ~16k данных flickr. 
Затем в течение следуюших 40 эпох обучалась на ~2450 данных мемов. 
Тут сразу хочется сказать, что я не ожидала, что оно будет обучаться до конца на мемах
(если в течение 10 эпох модель не улучшала bleu-3, она бы оставновила обучение). 
Но bleu-3 все улучшалось и улучшалось понемногу ([тут логи](output/baseline.log)), 
а модель все переобучалась и переобучалась. И в конце, все пришло к тому,
что она генерировала полуразумные описания, которые оставались такими до тех пор, 
пока не сравнишь с референсом и с картинкой - 
на самом деле описания модели это были перемешенные куски предложений из тренировочного датасета, 
никак не связанные с описанной картинкой. Получилось грустно, но видимо ожидаемо. 
А небольшое повышение bleu на каждом новом шаге обучения на мемах возможно можно объяснить тем, что
модель выучила общие паттерны описаний - такие как "на фото", "подпись: " и т.д., 
которые встречаются почти в каждом описании.

### Baseline 2: ResNet-101 + LSTM

Захотелось постараться исправить безйлайн хоть как-то прежде чем переходить к каким-то более сложным моделям. 
Что было исправлено:

1) Добавлено кол-во данных для обучения: использовался весь (переведенный) flickr - в трейне ~33k пар картинка-текст. К тому же, добавились данные VizWiz -  в трейне было N k пар.
2) Добавлены предобученные эмбеддинги (которые подавались на вход в лстм, раньше инициализировались рандомно) - взяла 
эмбеддинги из библиотеки navec (размерность 300) [отсюда](https://github.com/natasha/navec). В связи с этим, уменьшена размерность эмбеддингов картинки и текста, подаваемых на вход в LSTM до 300.
3) Увеличен кодировщик картинки с ResNet-50 до ResNet-101.
4) Увеличен дропаут в LSTM с 0.2 до 0.5
5) так же 30 эпох преобучаю сеть на переведенных данных Flickr, затем - максимум 20 эпох на мемах, но каждые 5 эпох сохраняю модель, чтобы затем выбрать лучшую по bleu (и по субъективным впечатлениям от ее генерации)
6) (не очень значительно, но) убрала токены \<start> и \<end>, потому что в процессе переписывания кода поняла, что они, кажется, бессмысленны здесь. Остались только специальные токены \<unk> и \<pad>.

Результаты: 

Примеры:


### Что дальше?


Возможно, сначала попробую еще обучить все то же самое, но с добавленным Attention между кодировщиком и декодировщиком.

Дальше, хочу все-таки попробовать использовать предобученные модели.

Пока цель - использовать идею из статьи [ClipCap: CLIP Prefix for Image Captioning](https://arxiv.org/abs/2111.09734). 
Основная идея - если есть предобученная модель CLIP и предобученная генеративная сеть, можем взять их и натренировать лишь
небольшую сетку MLP между ними - CLIP кодирует картинку, MLP как-то меняет эмбеддинг картинки и передает на вход 
генеративной сети, например, GPT, которая уже генерирует описание. На самом деле, авторы там рассказывают про несколько подходов к обучению такой связки:
обучение простой сетки можно заменить на обучение более сложной сети с вниманием, и можно дообучать/не дообучать GPT -
я скорее пойду по пути наименьшего расхода памяти и времени обучения, но если успею, может попробую разные варинты. 

Насчет самих моделей, у сбера как раз есть предобученный [ruCLIP](https://github.com/ai-forever/ru-clip) и [ruGPT](https://github.com/ai-forever/ru-gpts),
их и возьму. На самом деле, у меня даже есть почти готовый код для такого обучения [здесь](src/clip_model/train.py), надо лишь еще немного его поправить.


Есть еще один вариант, который я тоже немного попробовала уже запустить - это дообучить сберовский [RuDOLPH](https://github.com/ai-forever/ru-dolph) - 
большую модель мультимодальную модель, обученную выполнять множество задач, связанных с текстами и картинками. Одна 
из таких задач как раз Image Captioning, у них даже есть туториал по дообучению модели на для этой задачи. Но с первого раза
запустить у меня это не вышло, так как даже с батчем размера 1 память переполняется очень быстро и все падает (а это в колабе, где она больше, чем у меня на ноутбуке, на котором я обучаю модели).
Если получится разобраться в чем дело и хватит времени, попробую дообучить эту модель.
